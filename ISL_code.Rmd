---
title: "Introduction to Statistical Learning_Codes"
author: "Yifei Cao"
date: "12/11/2021"
output: 
  html_document:
    number_sections: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 4 _Classification_

## Lab: Logistic Regression, LDA, QDA, and KNN

### Logistic Regression

First, using **glm()** with the argument **family=binomial**, we could tell R to run a logistic regression.
```{r, warning=FALSE,message=FALSE}
library(ISLR)
attach(Smarket)

glm_fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
              data = Smarket, family = binomial) # family = binomial tells glm to run a logistic regression
summary(glm_fit)

```

Regarding the high p values (the smallest p is in Lag 1 and is 0.15), there is no evidence that Lag 1 is directly related to Direction

Therefore, we use the **coef()** functioin in order to access just the coefficients for this fitted model, and use **summary()** to access particular aspects of the model.

```{r, message=FALSE}
coef(glm_fit)
```
```{r, message=FALSE}
summary(glm_fit)$coef
```

The **predict()** function can be used to predict the probability that the market will go up, given values of the predictors. 
```{r}
glm_probs = predict(glm_fit, type = "response")
glm_pred = rep("Down", 1250) # a vector of 1250 Down elements
glm_pred[glm_probs > 0.5] = "Up"
table(glm_pred, Direction)
```

The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. The **mean()** functon can be used to compute the fraction of days for which the prediction was correct.

```{r}
mean(glm_pred == Direction)
```

Problems of this model is that we are using the same data sample to do both training and testing! So it is not appropriatem and cannot reflect the real preiction ability of the present model. Hence we select data from year 2001 to 2004 as training set, and year 2005 to be test set, to see how the model works.

```{r}
train = (Year<2005)
Smarket_2005 = Smarket[!train,]
Direction_2005 = Direction[!train]
#fit the logistic regression model using the training set
glm_fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
              data = Smarket, family = binomial, subset = train) # subset to subtract training set
glm_probs = predict(glm_fit, Smarket_2005, type = "response")

glm_pred = rep("Down", 252) #generate a vector for test set
glm_pred[glm_probs > 0.5] = "Up"
table(glm_pred, Direction_2005) #
```
```{r}
mean(glm_pred == Direction_2005)
```

This model looks worse than chance level!! What if we select those predictors that only have relatively lower p values and run the model again?

```{r}
train = (Year<2005)
Smarket_2005 = Smarket[!train,]
Direction_2005 = Direction[!train]
#fit the logistic regression model using the training set
glm_fit = glm(Direction ~ Lag1 + Lag2,
              data = Smarket, family = binomial, subset = train) # subset to subtract training set
glm_probs = predict(glm_fit, Smarket_2005, type = "response")

glm_pred = rep("Down", 252) #generate a vector for test set
glm_pred[glm_probs > 0.5] = "Up"
table(glm_pred, Direction_2005) #
```
```{r}
mean(glm_pred == Direction_2005)
```

### _Linear Discrininant Analysis_

Now, we are going to perform LDA on the **Smarket** data. In R, we fit a LDA model using the **lda()** function, which is part of the MASS library.

```{r}
library(MASS)
library(ISLR)
lda_fit = lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
lda_pred = predict(lda_fit, Smarket_2005)

lda_class = lda_pred$class
table(lda_class, Direction_2005)
```
```{r}
mean(lda_class == Direction_2005)
```

### _Quadratic Discriminant Analysis_

QDA is implemented in R using the **qda()** function,which is also part of the MASS library.

```{r}
qda_fit = qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
qda_class = predict(qda_fit, Smarket_2005)$class
table(qda_class, Direction_2005)
```

```{r}
mean(qda_class == Direction_2005)
```

The accuracy of QDA in this data sample is almost 60%!

### _K-Nearest Neighbors_

Now perform KNN using the knn() function, which is part of the class library. Unlike previous models, knn() forms predictions using a single command, and requires 4 inputs: (1) train.X, (2) test.X, (3) train.Y, (4) value for **K**, the number of nearest neighbors to be used by classifier.

First, modifying the data sample to the form of KNN uses.
```{r}
library(class)
# extracting the features used for KNN from the raw data
train_X = cbind(Lag1, Lag2)[train,]
test_X = cbind(Lag1, Lag2)[!train,]
train_Direction = Direction[train]
# ensure reproducibility of results, set random seeds
set.seed(1234)
knn_pred = knn(train_X, test_X, train_Direction, k = 1)
table(knn_pred, Direction_2005)
mean(knn_pred == Direction_2005)
```

### _An Application to Caravan Insurance Data_

This time we will apply KNN approach to the **Caravan** dataset. This data set contains 85 predictors, 5882 observations, and Purchase as variable (only 6% of people purchased caravan insurance).

In KNN, an annoying thing is that distance for $1000 in salary and difference for 50 years in age could be different, and this kind of difference could be also affected by the units they use. Therefore, we need to _standardize_ the data so that all variables are given a mean of zero and a sd of 1. The **scale()** function does this.



